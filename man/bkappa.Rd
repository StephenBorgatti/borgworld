% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/b_kappa.R
\name{bkappa}
\alias{bkappa}
\title{Compute Inter-Rater Reliability (Kappa Statistics)}
\usage{
bkappa(x, pairwise = TRUE)
}
\arguments{
\item{x}{A matrix where rows are documents (or units) and columns are coders.
Values should be 0/1 for binary coding, or category labels for nominal coding.}

\item{pairwise}{Logical. If TRUE, compute Cohen's kappa for all pairs of coders.
Default is TRUE.}
}
\value{
A list of class "bkappa" containing:
\item{fleiss}{Fleiss's kappa (overall agreement among all raters)}
\item{pct_agree}{Overall percent agreement (proportion of documents where all coders agree)}
\item{majority}{Vector of majority codes per document (NA if tied)}
\item{cohen}{Matrix of pairwise Cohen's kappa values (if pairwise=TRUE)}
\item{n_docs}{Number of documents}
\item{n_coders}{Number of coders}
\item{categories}{Unique categories found in the data}
}
\description{
Computes Fleiss's kappa for multiple raters and optionally Cohen's kappa
for all pairwise combinations. Also returns percent agreement and majority
codes per document.
}
\details{
Fleiss's kappa extends Cohen's kappa to multiple raters. Values range from
-1 to 1, where 1 indicates perfect agreement, 0 indicates agreement at
chance level, and negative values indicate agreement worse than chance.

Common interpretation guidelines (Landis & Koch, 1977):
\itemize{
\item < 0.00: Poor
\item 0.00-0.20: Slight
\item 0.21-0.40: Fair
\item 0.41-0.60: Moderate
\item 0.61-0.80: Substantial
\item 0.81-1.00: Almost perfect
}
}
\examples{
# 5 documents rated by 3 coders (binary: 0/1)
ratings <- matrix(c(
  1, 1, 1,   # Doc1: all agree (1)
  0, 0, 1,   # Doc2: 2 say 0, 1 says 1
  1, 1, 0,   # Doc3: 2 say 1, 1 says 0
  0, 0, 0,   # Doc4: all agree (0)
  1, 0, 1    # Doc5: 2 say 1, 1 says 0
), nrow = 5, byrow = TRUE,
   dimnames = list(paste0("Doc", 1:5), c("Alice", "Bob", "Carol")))

result <- bkappa(ratings)
result$fleiss
result$majority

}
\references{
Fleiss, J.L. (1971). Measuring nominal scale agreement among many raters.
Psychological Bulletin, 76(5), 378-382.

Cohen, J. (1960). A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1), 37-46.
}
